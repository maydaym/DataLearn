{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵补全\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MF():\n",
    "    def __init__(self, X, k, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict np.nan entries in a matrix.\n",
    "        Arguments\n",
    "        - X (ndarray)   : sample-feature matrix\n",
    "        - k (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        # True if not nan\n",
    "        self.not_nan_index = (np.isnan(self.X) == False)\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize factorization matrix U and V\n",
    "        self.U = np.random.normal(scale=1./self.k, size=(self.num_samples, self.k))\n",
    "        self.V = np.random.normal(scale=1./self.k, size=(self.num_features, self.k))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_samples)\n",
    "        self.b_v = np.zeros(self.num_features)\n",
    "        self.b = np.mean(self.X[np.where(self.not_nan_index)])\n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.X[i, j])\n",
    "            for i in range(self.num_samples)\n",
    "            for j in range(self.num_features)\n",
    "            if not np.isnan(self.X[i, j])\n",
    "        ]\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            # total square error\n",
    "            se = self.square_error()\n",
    "            training_process.append((i, se))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, se))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def square_error(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total square error\n",
    "        \"\"\"\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for i in range(self.num_samples):\n",
    "            for j in range(self.num_features):\n",
    "                if self.not_nan_index[i, j]:\n",
    "                    error += pow(self.X[i, j] - predicted[i, j], 2)\n",
    "        return error\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, x in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_x(i, j)\n",
    "            e = (x - prediction)\n",
    "\n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (2 * e - self.beta * self.b_u[i])\n",
    "            self.b_v[j] += self.alpha * (2 * e - self.beta * self.b_v[j])\n",
    "\n",
    "            # Update factorization matrix U and V\n",
    "            \"\"\"\n",
    "            If RuntimeWarning: overflow encountered in multiply,\n",
    "            then turn down the learning rate alpha.\n",
    "            \"\"\"\n",
    "            self.U[i, :] += self.alpha * (2 * e * self.V[j, :] - self.beta * self.U[i,:])\n",
    "            self.V[j, :] += self.alpha * (2 * e * self.U[i, :] - self.beta * self.V[j,:])\n",
    "\n",
    "    def get_x(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted x of sample i and feature j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_v[j] + self.U[i, :].dot(self.V[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, U and V\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:, np.newaxis] + self.b_v[np.newaxis, :] + self.U.dot(self.V.T)\n",
    "\n",
    "    def replace_nan(self, X_hat):\n",
    "        \"\"\"\n",
    "        Replace np.nan of X with the corresponding value of X_hat\n",
    "        \"\"\"\n",
    "        X = np.copy(self.X)\n",
    "        for i in range(self.num_samples):\n",
    "            for j in range(self.num_features):\n",
    "                if np.isnan(X[i, j]):\n",
    "                    X[i, j] = X_hat[i, j]\n",
    "        return X\n",
    "\n",
    "\n",
    "# 读取数据并处理数据\n",
    "datafile = u'E:\\\\桌面\\\\data_dict_source_7.19.xls'\n",
    "data = xlrd.open_workbook(datafile)\n",
    "table = data.sheets()[0]\n",
    "ncols = table.ncols\n",
    "nrows = table.nrows\n",
    "\n",
    "data2 = pd.DataFrame([])\n",
    "for i in range(ncols):\n",
    "    data2[i] = table.col_values(i)\n",
    "data2.rename(columns=data2.iloc[0, :], inplace=True)\n",
    "\n",
    "# 设置数据输出格式\n",
    "pd.set_option('display.max_columns', 20)       # 显示所有列\n",
    "pd.set_option('display.max_rows', 30)          # 显示所有行\n",
    "pd.set_option('max_colwidth', 100)             # 设置value的显示长度为100，默认为50\n",
    "\n",
    "data3 = data2\n",
    "# data3 = data2.drop(columns=[''])             # 删除第一列\n",
    "# drop函数中加入参数inplace=True表示对原表格修改\n",
    "# data2.columns  # 获取数据框列名\n",
    "data4 = data3.drop([0], axis=0)                # 删除第一行\n",
    "\n",
    "# print(data4)\n",
    "\n",
    "\n",
    "def mapping_1(data_text):\n",
    "    \"\"\"将文本数组转换为数值数组\"\"\"\n",
    "    data_text = list(data_text)\n",
    "    set_text = list(set(data_text))\n",
    "    data_num = pd.Series([])\n",
    "    for i in range(len(data_text)):\n",
    "        values = data_text[i]\n",
    "        data_num[i] = set_text.index(values)\n",
    "    return data_num\n",
    "\n",
    "\n",
    "# 将data4含文本数据框转换为数值数据框\n",
    "matrix_num = pd.DataFrame(columns=data4.columns)\n",
    "for i in range(data4.shape[1]):\n",
    "    matrix_num.iloc[:, i] = mapping_1(data4.iloc[:, i])\n",
    "\n",
    "# map(mapping_1, data4)  # map映射函数\n",
    "# print(matrix_num)\n",
    "\n",
    "\n",
    "h = 10\n",
    "xx = np.random.randint(matrix_num.shape[0], size=h)\n",
    "yy = np.random.randint(matrix_num.shape[1], size=h)\n",
    "for i in range(h):\n",
    "    matrix_num.iloc[xx[i], yy[i]] = np.nan\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = np.array(matrix_num, dtype=np.float)\n",
    "    # replace 0 with np.nan\n",
    "    print(X)\n",
    "    # np.random.seed(1)\n",
    "    mf = MF(X, k=5, alpha=0.1, beta=0.1, iterations=100)\n",
    "    mf.train()\n",
    "    X_hat = mf.full_matrix()\n",
    "    X_comp = mf.replace_nan(X_hat)\n",
    "\n",
    "    print(X_hat)\n",
    "    print(X_comp)\n",
    "    print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf判断文本相似度\n",
    "# -*- coding: utf-8 -*-\n",
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "data=pd.read_excel(r'C:\\Users\\Administrator\\Desktop\\0731复印机（原数据-标准值).xlsx')\n",
    "\n",
    "\n",
    "x=data['标配纸盒容量（原）']\n",
    "y=data['标配纸盒容量_y']\n",
    "\n",
    "data['标配纸盒容量（原）']=pd.DataFrame(data=newlist,columns=['标配纸盒容量（原）'])\n",
    "\n",
    "doc_test = doc_test.replace('（','').replace('）','').replace(',','').replace('：','').replace(';','')\n",
    "#print(doc_test)\n",
    "\n",
    "#训练模型的数据集 \n",
    "Traintest_word = []\n",
    "for word in x:\n",
    "    words_list = [words for words in jieba.cut(word)]   \n",
    "    Traintest_word.append(words_list)\n",
    "\n",
    "#print(Traintest_word)\n",
    "\n",
    "#测试用词\n",
    "\n",
    "doc_test = (\"350页（纸盒：250页；旁路：100页）,东芝（TOSHIBA）A3黑白复合机e-STUDIO2309A（主机+双面器+双面送稿器+第二纸盒+工作台）\")\n",
    "\n",
    "##删除特殊字符值\n",
    "doc_te = doc_test.replace('（','').replace('）','').replace(',','').replace('：','').replace(';','')\n",
    "# print(doc_test)\n",
    "\n",
    "#对测试数据进行jieba分词\n",
    "doc_test_list = [word for word in jieba.cut(doc_te)]\n",
    "#print(doc_test_list)\n",
    "\n",
    "#用dictionary方法获取词袋\n",
    "dictionary = corpora.Dictionary(Traintest_word)\n",
    "#print(dictionary)\n",
    "#词袋中用数字对所有词进行了编号\n",
    "a=dictionary.keys()\n",
    "#print(a)\n",
    "#使用doc2bow制作语料库，利用词袋模型中的字典将其映射到向量空间\n",
    "corpus = [dictionary.doc2bow(doc) for doc in Traintest_word]\n",
    "#print(corpus)\n",
    "\n",
    "#对测试文档也进行制作语料库，利用词袋模型中的字典将其映射到向量空间\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)\n",
    "#print(doc_test_vec)\n",
    "#使用TF-IDF模型对语料库建模\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "#print(tfidf)\n",
    "#获文档中，每个词的TF-IDF值 tfidf[corpus]\n",
    "#对每个目标文档，分析测试文档的相似度\n",
    "#model 实例化\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "#print(len(sim))\n",
    "#根据相似度排序是一个列表  表中每一项是一个元组   元组中前面是原句索引  后面是相似度\n",
    "SimilaritiesList = sorted(enumerate(sim), key=lambda item: -item[1])\n",
    "#print(SimilaritiesList)\n",
    "\n",
    "num = 0\n",
    "while(num<=5):\n",
    "    Result_tutple = SimilaritiesList[num]    #获取元组   索引  相似度\n",
    "    Result_index = Result_tutple[0]    #获取索引\n",
    "    #print(Traintest_word[Result_index])    # 输出分词后数值\n",
    "    print(Result_tutple,y[Result_index])\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
